{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51595832-52a2-4469-8aee-61428a86c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "#BATCH_SIZE = 2048\n",
    "#BATCH_SIZE = 512\n",
    "PAD_SIZE = 70\n",
    "TARGET_LABELS = [\"Solved\", \"Improvement\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6016c6-543d-4ede-b6dd-2a4870f324d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67cdfc5c-074f-4f7f-9f9d-39d47b43a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(3093453)\n",
    "\n",
    "sns.set(\n",
    "    context=\"talk\",\n",
    "    style=\"darkgrid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e0e78e-ece4-4159-aa86-3cd0d407c31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/fynn/Uni/MA'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd, _ = os.path.split(cwd)\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed6f3329-37c6-4683-ad68-5725ed932f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 199898 files belonging to 2 classes.\n",
      "Using 159919 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 20:15:23.478216: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-10-19 20:15:23.478253: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-10-19 20:15:23.478257: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-10-19 20:15:23.478296: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-19 20:15:23.478317: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 199898 files belonging to 2 classes.\n",
      "Using 39979 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "samples = os.path.join(cwd, \"data\", \"2D\", \"MIP\")\n",
    "class_names = [\"interrupted\", \"solved\"]\n",
    "\n",
    "train_dataset = keras.preprocessing.text_dataset_from_directory(\n",
    "    directory = samples,\n",
    "    labels = \"inferred\",\n",
    "    label_mode = \"binary\",\n",
    "    class_names = class_names,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    max_length = None,\n",
    "    shuffle = True,\n",
    "    seed = 3093453,\n",
    "    validation_split = 0.2,\n",
    "    subset = \"training\",\n",
    "    follow_links = False\n",
    ")\n",
    "\n",
    "val_dataset = keras.preprocessing.text_dataset_from_directory(\n",
    "    directory = samples,\n",
    "    labels = \"inferred\",\n",
    "    label_mode = \"binary\",\n",
    "    class_names = class_names,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    max_length = None,\n",
    "    shuffle = True,\n",
    "    seed = 3093453,\n",
    "    validation_split = 0.2,\n",
    "    subset = \"validation\",\n",
    "    follow_links = False\n",
    ")\n",
    "\n",
    "class_names = np.array(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67b7b15-cae4-480d-807b-11a180484459",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,y in train_dataset.take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a95fdba1-fe88-4950-b662-06ebf4803854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3408203\n"
     ]
    }
   ],
   "source": [
    "#y = pd.Series(X.numpy().astype(str)).str.extractall(\"MIP Improvement( )\")\n",
    "#y = y.groupby(y.index.get_level_values(0)).count().to_numpy()[:,0]\n",
    "#y = y == 1\n",
    "print(y.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66902747-b5e1-4f35-9f49-167482be7b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: C\n",
      "instance: MA\n",
      "+-----------+\n",
      "2D Packing MIP with Time Limit 4.3316688990958205 [s] in Truck P107037703\n",
      "Stack 0 with items: ['0090016200_27022023037676', '0090016200_27022023035725']\n",
      "Stack 1 with items: ['0090016200_27022023035144', '0090016200_27022023035144']\n",
      "Stack 2 with items: ['0090016200_27022023035144', '0090016200_27022023020174']\n",
      "Stack 3 with items: ['0090016200_27022023020174', '0090016200_27022023020174']\n",
      "Stack 4 with items: ['0090016200_27022023016615', '0090016200_27022023013329']\n",
      "Stack 5 with items: ['0090016200_27022023013329', '0090016200_27022023011216']\n",
      "Stack 6 with items: ['0090016200_27022023010543', '0090016200_27022023007284']\n",
      "Stack 7 with items: ['0090016200_27022023002073', '0090016200_27022023002073']\n",
      "MIP Improvement - 2D Vol: 16.5 [m2] - packed 2D Vol Ratio: 0.5009107468123862 [%] - after 0.0 [s] without stacks:\n",
      "Stack 4 missing: ['0090016200_27022023016615', '0090016200_27022023013329']\n",
      "Stack 5 missing: ['0090016200_27022023013329', '0090016200_27022023011216']\n",
      "Stack 6 missing: ['0090016200_27022023010543', '0090016200_27022023007284']\n",
      "Stack 7 missing: ['0090016200_27022023002073', '0090016200_27022023002073']\n",
      "Optimal Solution confirmed after 0.07156484201550484 [s]\n",
      "Stacks not packed:\n",
      "Stack 4 not in final solution with items: ['0090016200_27022023016615', '0090016200_27022023013329']\n",
      "Stack 5 not in final solution with items: ['0090016200_27022023013329', '0090016200_27022023011216']\n",
      "Stack 6 not in final solution with items: ['0090016200_27022023010543', '0090016200_27022023007284']\n",
      "Stack 7 not in final solution with items: ['0090016200_27022023002073', '0090016200_27022023002073']\n"
     ]
    }
   ],
   "source": [
    "df = pl.DataFrame({\"raw\": X.numpy().astype(str)})\n",
    "print(df[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b48de-cb45-4f62-99f0-9cf125c35048",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c26da7a-f0f1-4b99-b35f-519d9c9549df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 17)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Supplier code</th><th>Supplier dock</th><th>Plant code</th><th>Plant dock</th><th>Product code</th><th>Package code</th><th>Length</th><th>Width</th><th>Height</th><th>Weight</th><th>Nesting height</th><th>dataset</th><th>instance</th><th>item_id</th><th>ForcedLength</th><th>ForcedWidth</th><th>NestedHeight</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>bool</td><td>bool</td><td>f64</td></tr></thead><tbody><tr><td>2671602</td><td>null</td><td>90016900</td><td>&quot;169GD-60&quot;</td><td>&quot;777600002R&quot;</td><td>&quot;SFDA--6614&quot;</td><td>1.6</td><td>1.2</td><td>0.93</td><td>0.38658</td><td>0.045</td><td>&quot;C&quot;</td><td>&quot;BY&quot;</td><td>&quot;0090016900_270…</td><td>false</td><td>false</td><td>0.885</td></tr><tr><td>21792913</td><td>null</td><td>90016900</td><td>&quot;GRE1&quot;</td><td>&quot;255679073R&quot;</td><td>&quot;ECA---0021&quot;</td><td>1.2</td><td>1.0</td><td>0.435</td><td>0.031828</td><td>0.0</td><td>&quot;C&quot;</td><td>&quot;BY&quot;</td><td>&quot;0090016900_270…</td><td>false</td><td>false</td><td>0.435</td></tr><tr><td>679301</td><td>null</td><td>90016900</td><td>&quot;169GB-20&quot;</td><td>&quot;8200665427&quot;</td><td>&quot;MTEM--4462&quot;</td><td>2.4</td><td>1.2</td><td>1.62</td><td>0.457104</td><td>0.045</td><td>&quot;C&quot;</td><td>&quot;BY&quot;</td><td>&quot;0090016900_270…</td><td>false</td><td>false</td><td>1.575</td></tr><tr><td>27029901</td><td>null</td><td>90016900</td><td>&quot;169GD-60&quot;</td><td>&quot;769150010R&quot;</td><td>&quot;SFDA--6608&quot;</td><td>1.6</td><td>1.2</td><td>0.93</td><td>0.17098</td><td>0.045</td><td>&quot;C&quot;</td><td>&quot;BY&quot;</td><td>&quot;0090016900_270…</td><td>false</td><td>false</td><td>0.885</td></tr><tr><td>61199400</td><td>null</td><td>90016900</td><td>&quot;169GD-60&quot;</td><td>&quot;8200733103&quot;</td><td>&quot;SLI---0760&quot;</td><td>1.2</td><td>1.0</td><td>0.93</td><td>0.176252</td><td>0.045</td><td>&quot;C&quot;</td><td>&quot;BY&quot;</td><td>&quot;0090016900_270…</td><td>false</td><td>false</td><td>0.885</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 17)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ Supplier  ┆ Supplier  ┆ Plant     ┆ Plant     ┆ … ┆ item_id   ┆ ForcedLen ┆ ForcedWid ┆ NestedHe │\n",
       "│ code      ┆ dock      ┆ code      ┆ dock      ┆   ┆ ---       ┆ gth       ┆ th        ┆ ight     │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ str       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i64       ┆ str       ┆ i64       ┆ str       ┆   ┆           ┆ bool      ┆ bool      ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 2671602   ┆ null      ┆ 90016900  ┆ 169GD-60  ┆ … ┆ 009001690 ┆ false     ┆ false     ┆ 0.885    │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 0_2702202 ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 3000071   ┆           ┆           ┆          │\n",
       "│ 21792913  ┆ null      ┆ 90016900  ┆ GRE1      ┆ … ┆ 009001690 ┆ false     ┆ false     ┆ 0.435    │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 0_2702202 ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 3000077   ┆           ┆           ┆          │\n",
       "│ 679301    ┆ null      ┆ 90016900  ┆ 169GB-20  ┆ … ┆ 009001690 ┆ false     ┆ false     ┆ 1.575    │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 0_2702202 ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 3000086   ┆           ┆           ┆          │\n",
       "│ 27029901  ┆ null      ┆ 90016900  ┆ 169GD-60  ┆ … ┆ 009001690 ┆ false     ┆ false     ┆ 0.885    │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 0_2702202 ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 3000268   ┆           ┆           ┆          │\n",
       "│ 61199400  ┆ null      ┆ 90016900  ┆ 169GD-60  ┆ … ┆ 009001690 ┆ false     ┆ false     ┆ 0.885    │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 0_2702202 ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 3000285   ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"truck-item-infos\"\n",
    "file = \"items.csv\"\n",
    "\n",
    "item_path = os.path.join(cwd, folder, file)\n",
    "items = pl.read_csv(source = item_path);\n",
    "\n",
    "items = items.lazy().with_columns([\n",
    "    (pl.col(\"Item ident\").alias(\"item_id\")),\n",
    "    (pl.col(\"Length\") / 1000), # mm to m\n",
    "    (pl.col(\"Width\") / 1000),  # mm to m\n",
    "    (pl.col(\"Height\") / 1000), # mm to m\n",
    "    (pl.col(\"Nesting height\") / 1000), # mm to m\n",
    "    (pl.col(\"Weight\") / 1000), # kg to tons\n",
    "    (pl.col(\"Forced orientation\") == \"lengthwise\").alias(\"ForcedLength\"),\n",
    "    (pl.col(\"Forced orientation\") == \"widthwise\").alias(\"ForcedWidth\"),\n",
    "    ((pl.col(\"Height\") - pl.col(\"Nesting height\")) / 1000).alias(\"NestedHeight\"),\n",
    "    pl.col(\"dataset\").str.extract(\"([A-Z])\")\n",
    "]).drop([\n",
    "    \"Forced orientation\", \"Max stackability\", \"Inventory cost\",\n",
    "    \"Earliest arrival time\", \"Latest arrival time\", \"Number of items\",\n",
    "    \"instance_id\",\n",
    "    \"Stackability code\",\n",
    "    \"Item ident\" # drop because of rename earlier\n",
    "]).unique()\n",
    "\n",
    "items.collect().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dc63cfa-2e58-450c-8880-6b1bbfc96140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Supplier code</th><th>Supplier loading order</th><th>Supplier dock</th><th>Supplier dock loading order</th><th>Plant dock</th><th>Plant dock loading order</th><th>truck_id</th><th>dataset</th><th>instance</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>62059600</td><td>1</td><td>null</td><td>1</td><td>&quot;X0&quot;</td><td>1</td><td>&quot;P187721701&quot;</td><td>&quot;A&quot;</td><td>&quot;AS&quot;</td></tr><tr><td>29908700</td><td>1</td><td>null</td><td>1</td><td>&quot;X4&quot;</td><td>1</td><td>&quot;P187730401&quot;</td><td>&quot;A&quot;</td><td>&quot;AS&quot;</td></tr><tr><td>29908700</td><td>1</td><td>null</td><td>1</td><td>&quot;X4&quot;</td><td>1</td><td>&quot;P187730403&quot;</td><td>&quot;A&quot;</td><td>&quot;AS&quot;</td></tr><tr><td>29908700</td><td>1</td><td>null</td><td>1</td><td>&quot;X4&quot;</td><td>1</td><td>&quot;P187730405&quot;</td><td>&quot;A&quot;</td><td>&quot;AS&quot;</td></tr><tr><td>29908700</td><td>1</td><td>null</td><td>1</td><td>&quot;X4&quot;</td><td>1</td><td>&quot;P187730407&quot;</td><td>&quot;A&quot;</td><td>&quot;AS&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌────────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬─────────┬──────────┐\n",
       "│ Supplier   ┆ Supplier   ┆ Supplier  ┆ Supplier  ┆ … ┆ Plant     ┆ truck_id  ┆ dataset ┆ instance │\n",
       "│ code       ┆ loading    ┆ dock      ┆ dock      ┆   ┆ dock      ┆ ---       ┆ ---     ┆ ---      │\n",
       "│ ---        ┆ order      ┆ ---       ┆ loading   ┆   ┆ loading   ┆ str       ┆ str     ┆ str      │\n",
       "│ i64        ┆ ---        ┆ str       ┆ order     ┆   ┆ order     ┆           ┆         ┆          │\n",
       "│            ┆ i64        ┆           ┆ ---       ┆   ┆ ---       ┆           ┆         ┆          │\n",
       "│            ┆            ┆           ┆ i64       ┆   ┆ i64       ┆           ┆         ┆          │\n",
       "╞════════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═════════╪══════════╡\n",
       "│ 62059600   ┆ 1          ┆ null      ┆ 1         ┆ … ┆ 1         ┆ P18772170 ┆ A       ┆ AS       │\n",
       "│            ┆            ┆           ┆           ┆   ┆           ┆ 1         ┆         ┆          │\n",
       "│ 29908700   ┆ 1          ┆ null      ┆ 1         ┆ … ┆ 1         ┆ P18773040 ┆ A       ┆ AS       │\n",
       "│            ┆            ┆           ┆           ┆   ┆           ┆ 1         ┆         ┆          │\n",
       "│ 29908700   ┆ 1          ┆ null      ┆ 1         ┆ … ┆ 1         ┆ P18773040 ┆ A       ┆ AS       │\n",
       "│            ┆            ┆           ┆           ┆   ┆           ┆ 3         ┆         ┆          │\n",
       "│ 29908700   ┆ 1          ┆ null      ┆ 1         ┆ … ┆ 1         ┆ P18773040 ┆ A       ┆ AS       │\n",
       "│            ┆            ┆           ┆           ┆   ┆           ┆ 5         ┆         ┆          │\n",
       "│ 29908700   ┆ 1          ┆ null      ┆ 1         ┆ … ┆ 1         ┆ P18773040 ┆ A       ┆ AS       │\n",
       "│            ┆            ┆           ┆           ┆   ┆           ┆ 7         ┆         ┆          │\n",
       "└────────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴─────────┴──────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"trucks.csv\"\n",
    "\n",
    "truck_path = os.path.join(cwd, folder, file)\n",
    "\n",
    "truck_clms = [\"Id truck\", \"dataset\", \"instance\",\n",
    "              \"Supplier code\", \"Supplier dock\", \"Plant dock\",\n",
    "              \"Supplier loading order\", \"Supplier dock loading order\", \"Plant dock loading order\",\n",
    "             ]\n",
    "\n",
    "truck_stops = (\n",
    "    pl.read_csv(source = truck_path, columns = truck_clms)\n",
    "    .lazy()\n",
    "    .unique()\n",
    "    .rename({\"Id truck\": \"truck_id\"})\n",
    "    .sort([\"dataset\", \"instance\", \"truck_id\"])\n",
    "    .with_columns([pl.col(\"dataset\").str.extract(\"(\\w)\")])\n",
    ")\n",
    "truck_stops.collect().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e492480-4e8d-4534-8029-096c64ec1da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>truck_id</th><th>Length</th><th>Width</th><th>Weight</th><th>dataset</th><th>instance</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;P187721701&quot;</td><td>14.94</td><td>2.5</td><td>24.0</td><td>&quot;A&quot;</td><td>&quot;AS&quot;</td></tr><tr><td>&quot;P187730401&quot;</td><td>14.94</td><td>2.5</td><td>24.0</td><td>&quot;A&quot;</td><td>&quot;AS&quot;</td></tr><tr><td>&quot;P187730403&quot;</td><td>14.94</td><td>2.5</td><td>24.0</td><td>&quot;A&quot;</td><td>&quot;AS&quot;</td></tr><tr><td>&quot;P187730405&quot;</td><td>14.94</td><td>2.5</td><td>24.0</td><td>&quot;A&quot;</td><td>&quot;AS&quot;</td></tr><tr><td>&quot;P187730407&quot;</td><td>14.94</td><td>2.5</td><td>24.0</td><td>&quot;A&quot;</td><td>&quot;AS&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌────────────┬────────┬───────┬────────┬─────────┬──────────┐\n",
       "│ truck_id   ┆ Length ┆ Width ┆ Weight ┆ dataset ┆ instance │\n",
       "│ ---        ┆ ---    ┆ ---   ┆ ---    ┆ ---     ┆ ---      │\n",
       "│ str        ┆ f64    ┆ f64   ┆ f64    ┆ str     ┆ str      │\n",
       "╞════════════╪════════╪═══════╪════════╪═════════╪══════════╡\n",
       "│ P187721701 ┆ 14.94  ┆ 2.5   ┆ 24.0   ┆ A       ┆ AS       │\n",
       "│ P187730401 ┆ 14.94  ┆ 2.5   ┆ 24.0   ┆ A       ┆ AS       │\n",
       "│ P187730403 ┆ 14.94  ┆ 2.5   ┆ 24.0   ┆ A       ┆ AS       │\n",
       "│ P187730405 ┆ 14.94  ┆ 2.5   ┆ 24.0   ┆ A       ┆ AS       │\n",
       "│ P187730407 ┆ 14.94  ┆ 2.5   ┆ 24.0   ┆ A       ┆ AS       │\n",
       "└────────────┴────────┴───────┴────────┴─────────┴──────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"trucks.csv\"\n",
    "\n",
    "truck_path = os.path.join(cwd, folder, file)\n",
    "\n",
    "truck_clms = [\"Id truck\", \"dataset\", \"instance\",\n",
    "              \"Length\", \"Width\", \"Max weight\",\n",
    "             ]\n",
    "\n",
    "truck_dims = (\n",
    "    pl.read_csv(source = truck_path, columns = truck_clms)\n",
    "    .lazy()\n",
    "    .unique()\n",
    "    .rename({\"Id truck\": \"truck_id\", \"Max weight\": \"Weight\"})\n",
    "    .sort([\"dataset\", \"instance\", \"truck_id\"])\n",
    "    .with_columns([pl.col(\"dataset\").str.extract(\"(\\w)\")])\n",
    ")\n",
    "\n",
    "truck_dims = truck_dims.with_columns([\n",
    "    (pl.col(\"Length\") / 1000),\n",
    "    (pl.col(\"Width\") / 1000),\n",
    "    (pl.col(\"Weight\") / 1000),\n",
    "])\n",
    "\n",
    "truck_dims.collect().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d9335a9-2bd8-41f7-ba46-909dee42efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.preprocessing import extract_raw_data, explode_instances_into_stacks, explode_stacks_into_items\n",
    "from utils.preprocessing import join_items, group_items_by_stack, join_truck_loading_order, append_truck_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f191cc73-619b-4f7e-9f1d-2ed53bca5583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2.75, 1.5, ..., False, '2-2-1', False],\n",
       "       [0, 2.75, 1.5, ..., False, '2-2-1', False],\n",
       "       [0, 2.75, 1.5, ..., False, '2-2-1', False],\n",
       "       ...,\n",
       "       [1023, 1.2, 1.0, ..., False, '1-1-1', False],\n",
       "       [1023, 1.2, 1.0, ..., False, '1-1-1', False],\n",
       "       [1023, 14.94, 2.5, ..., False, '0-0-0', True]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = (\n",
    "    df.lazy()\n",
    "    .pipe(extract_raw_data)\n",
    "    .pipe(explode_instances_into_stacks)\n",
    "    .pipe(explode_stacks_into_items)\n",
    "    .pipe(join_items, items)\n",
    "    .pipe(group_items_by_stack)\n",
    "    .pipe(join_truck_loading_order, truck_stops)\n",
    "    .pipe(append_truck_info, truck_dims)\n",
    "    .collect()\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4c5abbd-dbdb-49a6-a47b-a8c47618b916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 70, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tensor_representation(X, packing_clm=6):\n",
    "    \n",
    "    indices = np.unique(X[:, 0])\n",
    "    indices = np.sort(indices)\n",
    "\n",
    "    # (batch_size, ?, features)\n",
    "    X = np.array([X[X[:,0] == idx] for idx in indices], dtype = \"object\")\n",
    "\n",
    "    # replace the packing order with the stop index (i.e 1-1-1 and 1-1-2 turn to 0 and 1, respectively)\n",
    "    #packing_clm = min([i for i, clm in enumerate(df.columns) if clm == \"packing_order\"])\n",
    "    \n",
    "    for i, x in enumerate(X):\n",
    "        packing_order = x[:,packing_clm]\n",
    "        stops = np.unique(packing_order)\n",
    "        stops = np.sort(stops)\n",
    "        stops = {stop: j for j, stop in enumerate(stops)}\n",
    "        stops = [stops[order] for order in packing_order]\n",
    "        X[i][:,packing_clm] = stops\n",
    "\n",
    "    # pad the variable length number of stacks into fixed\n",
    "    #  (batch_size, pad_size, features)\n",
    "    X = tf.keras.utils.pad_sequences(X, maxlen=PAD_SIZE, padding = \"post\", dtype=\"float64\")\n",
    "    # drop the index column (batch_size, pad_len, n_features)\n",
    "    X = X[:,:,1:].astype(float)\n",
    "    return X\n",
    "\n",
    "c = get_tensor_representation(X)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a99a4e-67bb-42ed-ac51-d1391e3825ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8  ,  0.6  ,  0.549,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 0.8  ,  0.6  ,  0.573,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 0.8  ,  0.6  ,  0.549,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 0.8  ,  0.6  ,  0.578,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 0.8  ,  0.6  ,  0.466,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 0.8  ,  0.6  ,  0.49 ,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 0.8  ,  0.6  ,  0.431,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 1.6  ,  1.2  ,  0.472,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 1.6  ,  1.2  ,  0.472,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 1.6  ,  1.2  ,  0.472,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 1.6  ,  1.2  ,  0.472,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  0.735,  0.   ,  0.   ,  4.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  0.755,  0.   ,  0.   ,  4.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  0.748,  0.   ,  0.   ,  4.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  0.742,  0.   ,  0.   ,  4.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  0.729,  0.   ,  0.   ,  4.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  0.486,  0.   ,  0.   ,  4.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  0.907,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  0.957,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  1.157,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  1.007,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  0.958,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  1.219,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 1.2  ,  1.   ,  0.433,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 1.6  ,  1.2  ,  0.737,  0.   ,  0.   ,  2.   ,  0.   ],\n",
       "       [ 1.206,  1.01 ,  0.636,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [ 1.206,  1.01 ,  0.318,  0.   ,  0.   ,  3.   ,  0.   ],\n",
       "       [13.5  ,  2.44 , 24.   ,  0.   ,  0.   ,  0.   ,  1.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[1, :, :].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dab19ec-58a3-4088-a89b-934e01616d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_labels(df: pl.DataFrame) -> dict[str: np.array]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = \"MIP Improvement - 2D Vol: \\d*\\.\\d* \\[m2\\] - packed 2D Vol Ratio\\: \\d*\\.\\d* \\[\\%\\] - after \\d*\\.\\d* \\[s\\]\"\n",
    "    mip_improvements = df[\"raw\"].str.extract_all(pattern)#.list[-1][2]\n",
    "\n",
    "    # mip_improvements: pl.Series[list[str]]\n",
    "    # with entries according to the pattern, i.e all MIP improvement rows\n",
    "    \n",
    "    y_num_improvements = mip_improvements.list.len()-1\n",
    "    \n",
    "    y_improvement = y_num_improvements > 0\n",
    "\n",
    "    y_packed_area_ratio = mip_improvements.list[-1].str.extract(\"\\: (\\d*\\.\\d*) \\[\\%\\]\").cast(pl.Float32)\n",
    "\n",
    "    y_packed_area = mip_improvements.list[-1].str.extract(\"- 2D Vol: (\\d*\\.\\d*) \\[m2\\]\").cast(pl.Float32)\n",
    "\n",
    "    y_last_update = mip_improvements.list[-1].str.extract(\"- after (\\d*\\.\\d*) \\[s\\]\").cast(pl.Float32)\n",
    "\n",
    "    y = [\n",
    "        #y_num_improvements.to_numpy(),\n",
    "        y_improvement.to_numpy().astype(int),\n",
    "        #y_packed_area_ratio.to_numpy(),\n",
    "        #y_packed_area.to_numpy(),\n",
    "        #y_last_update.to_numpy()\n",
    "    ]\n",
    "    \n",
    "    return y\n",
    "\n",
    "#pattern = \"Stack (\\d*) not in final solution with items:\"\n",
    "#df[\"raw\"].str.extract_all(pattern).apply(lambda x: [i.split(\" \")[1] for i in x])\n",
    "#x = df[\"raw\"].str.extract_all(pattern).map_elements(lambda x: [int(i.split(\" \")[1]) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5171b197-a4d2-4623-b31a-d12c1bf13170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = \"Stack (\\d*) not in final solution with items:\"\n",
    "#df[\"raw\"].str.extract_all(pattern).apply(lambda x: [i.split(\" \")[1] for i in x])\n",
    "#x = df[\"raw\"].str.extract_all(pattern).map_elements(lambda x: [int(i.split(\" \")[1]) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c124bd32-2e6d-49c5-8860-26ab6cb216f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polars_transformation(X_batch, y_batch) -> (np.array, np.array):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X: np.array[float32]\n",
    "        3D Feature Tensor of shape (Batch_size, Pad_size, n_features=7)\n",
    "\n",
    "        - Batch_size: Truck Optimization Instances\n",
    "        - Pad_size: Stacks (or Trucks), padded up to create tensors\n",
    "        - n_features: Length, Width, Weight, L/W Forced Orientation\n",
    "                      packing order, is_truck\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pl.DataFrame({\"raw\": X_batch.numpy().astype(str)})\n",
    "    \n",
    "    X = (\n",
    "        df.lazy()\n",
    "        .pipe(extract_raw_data)\n",
    "        .pipe(explode_instances_into_stacks)\n",
    "        .pipe(explode_stacks_into_items)\n",
    "        .pipe(join_items, items)\n",
    "        .pipe(group_items_by_stack)\n",
    "        .pipe(join_truck_loading_order, truck_stops)\n",
    "        .pipe(append_truck_info, truck_dims)\n",
    "        .collect()\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "    X = get_tensor_representation(X)\n",
    "\n",
    "    y_batch = y_batch.numpy()\n",
    "    y_batch = [y_batch]\n",
    "    y_extra = get_additional_labels(df)\n",
    "    y_batch += y_extra\n",
    "\n",
    "    \n",
    "    return X, y_batch\n",
    "\n",
    "#polars_transformation(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b05dd03-d074-4b11-9ac6-5cf34e1fc0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: C\n",
      "instance: MA\n",
      "+-----------+\n",
      "2D Packing MIP with Time Limit 4.3316688990958205 [s] in Truck P107037703\n",
      "Stack 0 with items: ['0090016200_27022023037676', '0090016200_27022023035725']\n",
      "Stack 1 with items: ['0090016200_27022023035144', '0090016200_27022023035144']\n",
      "Stack 2 with items: ['0090016200_27022023035144', '0090016200_27022023020174']\n",
      "Stack 3 with items: ['0090016200_27022023020174', '0090016200_27022023020174']\n",
      "Stack 4 with items: ['0090016200_27022023016615', '0090016200_27022023013329']\n",
      "Stack 5 with items: ['0090016200_27022023013329', '0090016200_27022023011216']\n",
      "Stack 6 with items: ['0090016200_27022023010543', '0090016200_27022023007284']\n",
      "Stack 7 with items: ['0090016200_27022023002073', '0090016200_27022023002073']\n",
      "MIP Improvement - 2D Vol: 16.5 [m2] - packed 2D Vol Ratio: 0.5009107468123862 [%] - after 0.0 [s] without stacks:\n",
      "Stack 4 missing: ['0090016200_27022023016615', '0090016200_27022023013329']\n",
      "Stack 5 missing: ['0090016200_27022023013329', '0090016200_27022023011216']\n",
      "Stack 6 missing: ['0090016200_27022023010543', '0090016200_27022023007284']\n",
      "Stack 7 missing: ['0090016200_27022023002073', '0090016200_27022023002073']\n",
      "Optimal Solution confirmed after 0.07156484201550484 [s]\n",
      "Stacks not packed:\n",
      "Stack 4 not in final solution with items: ['0090016200_27022023016615', '0090016200_27022023013329']\n",
      "Stack 5 not in final solution with items: ['0090016200_27022023013329', '0090016200_27022023011216']\n",
      "Stack 6 not in final solution with items: ['0090016200_27022023010543', '0090016200_27022023007284']\n",
      "Stack 7 not in final solution with items: ['0090016200_27022023002073', '0090016200_27022023002073']\n"
     ]
    }
   ],
   "source": [
    "print(df[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1bff52-d516-4e59-adc3-dc6879865197",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba3a13-e18c-42b5-a652-e98181cfadef",
   "metadata": {},
   "source": [
    "## Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb0d9d3a-8089-45e8-8510-d5692a1239f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Layer Dropout of 11.2% resulting in a total Dropout of 30.0% over 3 encoders\n"
     ]
    }
   ],
   "source": [
    "N_FEATURES = c.shape[-1]\n",
    "\n",
    "# number of encoder blocks\n",
    "n_blocks = 3 #6\n",
    "num_heads = 4\n",
    "key_dim = int(N_FEATURES/num_heads)\n",
    "use_bias = True # If true the convergence seems to be much faster/sooner. But with false, it picks up pace later\n",
    "## Encoder dropout in total and per layer\n",
    "total_encoder_dropout = 0.3\n",
    "encoder_dropout = 1-(1-total_encoder_dropout)**(1/n_blocks)\n",
    "print(f\"Encoder Layer Dropout of {encoder_dropout:.1%} resulting in a total Dropout of {total_encoder_dropout:.1%} over {n_blocks} encoders\")\n",
    "\n",
    "\n",
    "# number of fully connected layers with droput\n",
    "#fully_connected_units = [] # [16]\n",
    "#fully_connected_dropout_rate = 0.5\n",
    "\n",
    "# activation function\n",
    "activation = \"gelu\" # \"relu\" \"gelu\" \"selu\" \"swish\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Learning Rate\n",
    "initial_lr = 10e-3 # 10e-3\n",
    "lr_decay = 0.9 # 0.9\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = initial_lr)\n",
    "#optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# Loss Function\n",
    "#loss_fn = tf.keras.losses.BinaryCrossentropy(name=\"binary_crossentropy\",)\n",
    "\n",
    "# Focal Loss gamma:\n",
    "\n",
    "loss_solved = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    gamma=2,\n",
    "    alpha = 1-y.numpy().mean(),\n",
    "    name='solved_focal_loss'\n",
    ")\n",
    "\n",
    "loss_improvement = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    gamma=1.5,\n",
    "    alpha = 0.56,\n",
    "    name='improvement_focal_loss'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81273159-358d-465f-ba74-d6ea72a07e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_objects = [\n",
    "    loss_solved,\n",
    "    loss_improvement\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90033cb5-930e-447d-b6e6-b0a917dc15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_losses = [tf.keras.metrics.Mean(name=target) for target in TARGET_LABELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e3e5121-bd70-407e-89eb-d7cae6c21c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics: list[list[tf.keras.metrics]] = [\n",
    "    [\n",
    "        tf.keras.metrics.Precision(name = \"PRC\"),\n",
    "        tf.keras.metrics.Recall(name = \"SNS\"),\n",
    "        tf.keras.metrics.AUC(curve='PR', name=\"AUC\"),\n",
    "        tf.keras.metrics.BinaryAccuracy(name = \"ACC\")\n",
    "    ],\n",
    "    [\n",
    "        tf.keras.metrics.Precision(name = \"PRC\"),\n",
    "        tf.keras.metrics.Recall(name = \"SNS\"),\n",
    "        tf.keras.metrics.AUC(curve='PR', name=\"AUC\"),\n",
    "        tf.keras.metrics.BinaryAccuracy(name = \"ACC\")\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2172818-ab30-45a6-a332-0062ac6574e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b5ba812-571a-4e1e-bedc-ce5bb0abd56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Masking, BatchNormalization, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.layers import Add, Dense, Input, Reshape, Permute, Lambda\n",
    "\n",
    "from utils.DNN.model_layers import TransformerEncoder\n",
    "\n",
    "#help(Masking)\n",
    "#help(MultiHeadAttention)\n",
    "#help(Reshape)\n",
    "#help(Dropout)\n",
    "#help(Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b3f6f99-9f5c-4a06-98d8-a9b1b199fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model using the functional API:\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "# Input\n",
    "input_layer = Input(shape=(PAD_SIZE, N_FEATURES))\n",
    "\n",
    "# Masking padded input\n",
    "masking_layer = Masking(\n",
    "    mask_value=0,\n",
    "    input_shape=(PAD_SIZE, N_FEATURES),\n",
    "    dtype=tf.float16, # float, # tf.float16\n",
    "    name=\"MaskingLayer\"\n",
    ")\n",
    "x = masking_layer(input_layer)\n",
    "\n",
    "#batch_norm_layer = BatchNormalization(name=\"BatchNormalizationLayer\")\n",
    "#x = batch_norm_layer(masking_layer)\n",
    "\n",
    "\n",
    "# Encoder Block\n",
    "for i in range(1, n_blocks+1):\n",
    "    # Self attention with add and layer norm\n",
    "    self_attention_layer = TransformerEncoder(\n",
    "        num_heads=num_heads, key_dim=key_dim,\n",
    "        dropout=encoder_dropout,               # Hyperparameter\n",
    "        use_bias=use_bias,                     # usually False, but technically Hyperparameter\n",
    "        idx = i,\n",
    "        activation=activation,\n",
    "        units = N_FEATURES,\n",
    "        use_PreLN=True\n",
    "    )\n",
    "    x = self_attention_layer(\n",
    "        x\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# After Attention, reduce to single dimension\n",
    "add_across_dim = Lambda(\n",
    "    lambda x: K.sum(x, axis=1)/PAD_SIZE**1,\n",
    "    output_shape=lambda s: (s[0], s[2]),\n",
    "    name = \"ReduceViaSum\"\n",
    ")\n",
    "\n",
    "x = add_across_dim(x)\n",
    "\n",
    "\n",
    "# Fully Connected Layer + Droput\n",
    "#for i, units in enumerate(fully_connected_units):\n",
    "#    \n",
    "#    full_connected_layer = Dense(units=units, activation=activation,name=f\"FullyConnectedLayer-{i+1}\")\n",
    "#    x = full_connected_layer(x)\n",
    "#\n",
    "#    dropout_layer = Dropout(fully_connected_dropout_rate,\n",
    "#                            name = f\"FullyConnectedDropoutLayer-{i+1}\")\n",
    "#    x = dropout_layer(x)\n",
    "\n",
    "reshape = Lambda(lambda x: tf.squeeze(x), name=\"Output\")\n",
    "\n",
    "\n",
    "\n",
    "full_connected_layer = Dense(units=N_FEATURES, activation=activation,name=f\"FullyConnectedLayerSolved\")\n",
    "xx = full_connected_layer(x)\n",
    "output_solved = Dense(1, activation='sigmoid', name = \"PredictionSolved\")\n",
    "xx = output_solved(xx)\n",
    "output_solved = reshape(xx)\n",
    "\n",
    "\n",
    "\n",
    "full_connected_layer = Dense(units=N_FEATURES, activation=activation,name=f\"FullyConnectedLayerImprovement\")\n",
    "xx = full_connected_layer(x)\n",
    "output_improvement = Dense(1, activation='sigmoid', name = \"PredictionImprovement\")\n",
    "xx = output_improvement(xx)\n",
    "output_improvement = reshape(xx)\n",
    "\n",
    "\n",
    "outputs = [output_solved, output_improvement]\n",
    "\n",
    "model = Model(\n",
    "    inputs=input_layer,\n",
    "    #outputs=x,\n",
    "    outputs=outputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1c0313b-3fe0-498b-925f-3547fcd860af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 70, 7)]              0         []                            \n",
      "                                                                                                  \n",
      " MaskingLayer (Masking)      (None, 70, 7)                0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " transformer_encoder (Trans  (None, 70, 7)                201       ['MaskingLayer[0][0]']        \n",
      " formerEncoder)                                                                                   \n",
      "                                                                                                  \n",
      " transformer_encoder_1 (Tra  (None, 70, 7)                201       ['transformer_encoder[0][0]'] \n",
      " nsformerEncoder)                                                                                 \n",
      "                                                                                                  \n",
      " transformer_encoder_2 (Tra  (None, 70, 7)                201       ['transformer_encoder_1[0][0]'\n",
      " nsformerEncoder)                                                   ]                             \n",
      "                                                                                                  \n",
      " ReduceViaSum (Lambda)       (None, 7)                    0         ['transformer_encoder_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " FullyConnectedLayerSolved   (None, 7)                    56        ['ReduceViaSum[0][0]']        \n",
      " (Dense)                                                                                          \n",
      "                                                                                                  \n",
      " FullyConnectedLayerImprove  (None, 7)                    56        ['ReduceViaSum[0][0]']        \n",
      " ment (Dense)                                                                                     \n",
      "                                                                                                  \n",
      " PredictionSolved (Dense)    (None, 1)                    8         ['FullyConnectedLayerSolved[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " PredictionImprovement (Den  (None, 1)                    8         ['FullyConnectedLayerImproveme\n",
      " se)                                                                nt[0][0]']                    \n",
      "                                                                                                  \n",
      " Output (Lambda)             None                         0         ['PredictionSolved[0][0]',    \n",
      "                                                                     'PredictionImprovement[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 731 (2.86 KB)\n",
      "Trainable params: 731 (2.86 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_ = model(c) # just check if it works\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dfadaf-029c-49d3-81f3-664967ff38b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training Process Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38f32c2a-d796-4649-8da9-34dedad77da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(history: pl.DataFrame, observe:str=\"Val-Loss\", patience:int=3, increase:bool=False) -> bool:\n",
    "    \"\"\"\n",
    "    Stop early if the last x=patience metrics of interest have decreased\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    history: DataFrame\n",
    "        dataframe with performance metrics, including the one of interest\n",
    "        \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stop_early: bool\n",
    "        indicates if we should stop the model trainig early\n",
    "    \"\"\"\n",
    "    assert hasattr(history, \"columns\") # dataframe of some sort\n",
    "    assert observe in history.columns  #\n",
    "    \n",
    "    diff = (history[observe] - history[observe].shift(1))\n",
    "    diff = diff.tail(patience) # only last x=patience epochs are relevant\n",
    "\n",
    "    if increase:\n",
    "        stop_early:bool = all(diff > 0)\n",
    "    else:\n",
    "        stop_early:bool = all(diff <= 0)\n",
    "    \n",
    "    return stop_early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee2627ee-d8be-44e4-aa07-332e6ce18de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping_no_improvement(history: pl.DataFrame, observe:str=\"Val-Loss\", patience:int=3, increase:bool=False) -> bool:\n",
    "    \"\"\"\n",
    "    Stop early if the last x=patience metrics of interest have decreased\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    history: DataFrame\n",
    "        dataframe with performance metrics, including the one of interest\n",
    "        \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stop_early: bool\n",
    "        indicates if we should stop the model trainig early\n",
    "    \"\"\"\n",
    "    assert hasattr(history, \"columns\") # dataframe of some sort\n",
    "    assert observe in history.columns  #\n",
    "    \n",
    "    x = history[observe]\n",
    "\n",
    "    if increase:\n",
    "        latest_improvement = x.arg_max()\n",
    "    else:\n",
    "        latest_improvement = x.arg_min()\n",
    "\n",
    "    latest_improvement = len(x) - latest_improvement\n",
    "\n",
    "    stop_early = latest_improvement > patience\n",
    "    \n",
    "    return stop_early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1911b3da-221c-4414-9358-7fee5a42a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_decay(epoch: int, initial_lr:float=10e-3, lr_decay:float=0.9) -> float:\n",
    "    \"\"\"\n",
    "    Exponential decay learning rate schedule\n",
    "    \"\"\"\n",
    "    return initial_lr * lr_decay**lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf804408-85ea-4824-8a74-fb9feb1807f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, losses, all_metrics=None, training:bool=True):\n",
    "    \n",
    "    metrics = \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\".join([f\"{TARGET_LABELS[i]:<12}\\tLoss: {losses[i].result():.3f}\" + \"\\t\".join([f\"{m.name:>10}: {m.result():.2%}\" for m in metrics])\n",
    "                                         for i, metrics in enumerate(all_metrics)])\n",
    "    \n",
    "    if training:\n",
    "        prefix=\"Training\"\n",
    "    else:\n",
    "        prefix=\"Validation\"\n",
    "    \n",
    "    print(f\"\\r{prefix}-Iteration: {iteration+1:0>3}/{total:<1000}\" + metrics,\n",
    "          end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71f44dd2-a301-4c37-919b-5a93fbe6c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def model_pass(inputs, targets, training):\n",
    "    \"\"\"\n",
    "    Usual Tensorflow model passing of inputs throught the network.\n",
    "    If in training mode, the optimizier can apply the gradients\n",
    "    observed with GradientTape to the model parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        outputs = model(inputs, training=training)\n",
    "        losses = [l(t, o) for l,o,t in zip(loss_objects, outputs, targets)]\n",
    "        \n",
    "        if training:\n",
    "            gradients = tape.gradient(losses, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        for i, mean_loss in enumerate(mean_losses):\n",
    "            mean_loss(losses[i])\n",
    "        \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "555d2a8e-ff78-42a9-871c-fabbc1da5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataset, all_metrics:list[list[tf.keras.metrics]], epoch:int, history:pd.DataFrame, training:bool=True):\n",
    "    \"\"\"\n",
    "    Single epoch, running several training steps over a dataset\n",
    "    \"\"\"\n",
    "\n",
    "    for step, (X_batch, y_batch) in enumerate(dataset):\n",
    "        \n",
    "        X_batch, y_batch = polars_transformation(X_batch, y_batch)\n",
    "        \n",
    "        outputs = model_pass(X_batch, y_batch, training=training)\n",
    "        #mean_loss(loss)\n",
    "        \n",
    "        for i, metrics in enumerate(all_metrics):\n",
    "            for metric in metrics:\n",
    "                metric(y_batch[i].reshape(-1), outputs[i].numpy().reshape(-1))\n",
    "    \n",
    "        print_status_bar(step, len(dataset), mean_losses, all_metrics, training=training)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    for i, metrics in enumerate(all_metrics):\n",
    "        for metric in metrics:\n",
    "            data = pd.DataFrame({\n",
    "                \"Epoch\": [epoch],\n",
    "                \"Target\": [TARGET_LABELS[i]],\n",
    "                \"Metric\": [metric.name if training else f\"Val-\"+metric.name],\n",
    "                \"Value\": [float(metric.result())],\n",
    "            })\n",
    "            history = pd.concat([history, data])\n",
    "            metric.reset_states()\n",
    "\n",
    "    for i, mean_loss in enumerate(mean_losses):\n",
    "        data = pd.DataFrame({\n",
    "            \"Epoch\": [epoch],\n",
    "            \"Target\": [TARGET_LABELS[i]],\n",
    "            \"Metric\": [\"Loss\" if training else f\"Val-\"+\"Loss\"],\n",
    "            \"Value\": [float(mean_loss.result())],\n",
    "        })\n",
    "        history = pd.concat([history, data])\n",
    "        mean_loss.reset_states()\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ef915-1e01-42dd-b423-71593e828fcf",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464563b0-8ebe-4448-9105-a25ed5e36383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 20:15:39.220087: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Iteration: 157/157                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Solved      \tLoss: 0.141       PRC: 70.13%\t       SNS: 50.19%\t       AUC: 68.75%\t       ACC: 74.69%\t\t\t\t\t\t\t\t\tImprovement \tLoss: 0.237       PRC: 56.95%\t       SNS: 45.52%\t       AUC: 55.90%\t       ACC: 62.06%%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/qmf6qmvd1gv0nctgx3m6jm3r0000gn/T/ipykernel_65041/4137319759.py:29: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  history = pd.concat([history, data])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation-Iteration: 040/40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Solved      \tLoss: 0.117       PRC: 82.34%\t       SNS: 55.65%\t       AUC: 79.00%\t       ACC: 79.74%\t\t\t\t\t\t\t\t\tImprovement \tLoss: 0.218       PRC: 58.33%\t       SNS: 68.38%\t       AUC: 63.67%\t       ACC: 65.61%\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 002/003\n",
      "Training-Iteration: 157/157                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Solved      \tLoss: 0.115       PRC: 85.98%\t       SNS: 52.71%\t       AUC: 79.56%\t       ACC: 80.13%\t\t\t\t\t\t\t\t\tImprovement \tLoss: 0.214       PRC: 63.16%\t       SNS: 57.23%\t       AUC: 65.92%\t       ACC: 67.49%\n",
      "\n",
      "Validation-Iteration: 040/40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Solved      \tLoss: 0.123       PRC: 68.09%\t       SNS: 66.79%\t       AUC: 80.67%\t       ACC: 76.79%\t\t\t\t\t\t\t\t\tImprovement \tLoss: 0.212       PRC: 62.78%\t       SNS: 63.20%\t       AUC: 67.81%\t       ACC: 68.26%\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 003/003\n",
      "Training-Iteration: 046/157                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Solved      \tLoss: 0.113       PRC: 86.22%\t       SNS: 53.57%\t       AUC: 80.35%\t       ACC: 80.43%\t\t\t\t\t\t\t\t\tImprovement \tLoss: 0.213       PRC: 64.45%\t       SNS: 57.01%\t       AUC: 67.75%\t       ACC: 68.12%"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "n_steps = len(train_dataset) // BATCH_SIZE\n",
    "\n",
    "clms = [\"Epoch\", \"Target\", \"Metric\", \"Value\"]\n",
    "history = pd.DataFrame(columns = clms)\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    print(f\"Epoch {epoch:0>3}/{ n_epochs:0>3}\")\n",
    "\n",
    "    history = run_epoch(train_dataset, all_metrics, epoch=epoch, history=history, training=True)\n",
    "    history = run_epoch(val_dataset, all_metrics, epoch=epoch, history=history, training=False)\n",
    "\n",
    "    print(\"-\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c961887-1831-43e7-98a7-51b71bf3bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history.pivot_table(values = [\"Value\"], columns=[\"Metric\"], index=[\"Epoch\", \"Target\"]).reset_index().set_index(\"Epoch\", drop=True)\n",
    "clms = history.columns\n",
    "history.columns = [clm[i>=1] for i, clm in enumerate(clms)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb1501-21f7-4ef2-96a7-af4205bf0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "clms = history.columns\n",
    "\n",
    "fig, ax = plt.subplots(len(TARGET_LABELS), 2, figsize=(16, 9*len(TARGET_LABELS)))\n",
    "\n",
    "colors = [\"cyan\", \"orange\", \"forestgreen\", \"purple\"]\n",
    "\n",
    "for i, target in enumerate(TARGET_LABELS):\n",
    "    history.loc[(history.Target == target), (~clms.str.contains(\"Val-\"))&(~clms.str.contains(\"Loss\"))]\\\n",
    "        .plot(ax=ax[i, 0], ls = \"--\", color = colors, label = \"Training\")\n",
    "    history.loc[(history.Target == target), (clms.str.contains(\"Val-\"))&(~clms.str.contains(\"Loss\"))]\\\n",
    "        .plot(ax=ax[i, 0], lw = 3, color = colors, label = \"Validation\")\n",
    "    ax[i, 0].set(ylabel = target)\n",
    "\n",
    "    history.loc[(history.Target == target), (~clms.str.contains(\"Val-\"))&(clms.str.contains(\"Loss\"))]\\\n",
    "        .plot(ax=ax[i, 1], ls = \"--\", color = \"red\", label = \"Training\")\n",
    "    history.loc[(history.Target == target), (clms.str.contains(\"Val-\"))&(clms.str.contains(\"Loss\"))]\\\n",
    "        .plot(ax=ax[i, 1], lw = 3, color = \"red\", label = \"Validation\")\n",
    "    #ax[i, 1].set(ylabel = target)\n",
    "\n",
    "ax[0, 0].set(title=\"Metrics\")\n",
    "ax[0, 1].set(title=\"Loss\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb1d76-f599-4e84-97b5-2c0c76f86adf",
   "metadata": {},
   "source": [
    "Tradeoff between Sensitivity and Precision can be seen very good here.\n",
    "Most likely explanation:\n",
    "- the bias of the final layer before the sigmoid has changed drastically.\n",
    "- if the bias increases, predicted probabilities increase. This means more found solved cases and higher Sensitivity\n",
    "- if the bias decreases, predicted probabilities decrease. This means less found solved cases, only the more certain cases. Higher Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faec77f-ae28-426c-9a59-867235bddd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    #to_file,\n",
    "    show_shapes=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
