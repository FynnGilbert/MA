{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51595832-52a2-4469-8aee-61428a86c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "\n",
    "PAD_SIZE = 80\n",
    "TARGET_LABELS = [\n",
    "    \"Solved\",\n",
    "    \"Improvement\",\n",
    "    \"Stacks\"\n",
    "]\n",
    "\n",
    "SEED = 3093453"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6016c6-543d-4ede-b6dd-2a4870f324d6",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d07e7-6859-4737-9c4b-91eba870632b",
   "metadata": {},
   "source": [
    "## Load Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67cdfc5c-074f-4f7f-9f9d-39d47b43a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(\n",
    "    edgeitems=30,\n",
    "    linewidth=100_000,\n",
    "    #formatter=dict(float=lambda x:\"%.3g\" % x),\n",
    "    suppress=True\n",
    "    )\n",
    "#np.core.arrayprint._line_width = 280\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "sns.set(\n",
    "    context=\"talk\",\n",
    "    style=\"darkgrid\"\n",
    ")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "cwd, _ = os.path.split(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6f3329-37c6-4683-ad68-5725ed932f53",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `class_names` passed did not match the names of the subdirectories of the target directory. Expected: [], but received: ['interrupted', 'solved']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m test_samples \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cwd, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2D\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMIP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m class_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterrupted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolved\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mtext_dataset_from_directory(\n\u001b[1;32m     23\u001b[0m     directory \u001b[38;5;241m=\u001b[39m train_samples,\n\u001b[1;32m     24\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     follow_links \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m class_names \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(class_names)\n",
      "File \u001b[0;32m~/Uni/MA/.venv/lib/python3.11/site-packages/keras/src/utils/text_dataset.py:161\u001b[0m, in \u001b[0;36mtext_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, batch_size, max_length, shuffle, seed, validation_split, subset, follow_links)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m file_paths, labels, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m     )\n",
      "File \u001b[0;32m~/Uni/MA/.venv/lib/python3.11/site-packages/keras/src/utils/dataset_utils.py:552\u001b[0m, in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mset\u001b[39m(subdirs):\n\u001b[0;32m--> 552\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    553\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `class_names` passed did not match the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames of the subdirectories of the target directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    555\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubdirs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m             )\n\u001b[1;32m    557\u001b[0m class_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(class_names, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(class_names))))\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Build an index of the files\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# in the different class subfolders.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The `class_names` passed did not match the names of the subdirectories of the target directory. Expected: [], but received: ['interrupted', 'solved']"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "train_samples = os.path.join(cwd, \"data\", \"train\", \"2D\", \"MIP\")\n",
    "test_samples = os.path.join(cwd, \"data\", \"test\", \"2D\", \"MIP\")\n",
    "class_names = [\"interrupted\", \"solved\"]\n",
    "\n",
    "train_dataset = keras.preprocessing.text_dataset_from_directory(\n",
    "    directory = train_samples,\n",
    "    labels = \"inferred\",\n",
    "    label_mode = \"binary\",\n",
    "    class_names = class_names,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    max_length = None,\n",
    "    shuffle = True,\n",
    "    seed = SEED,\n",
    "    validation_split = 0.1,\n",
    "    subset = \"training\",\n",
    "    follow_links = False\n",
    ")\n",
    "\n",
    "val_dataset = keras.preprocessing.text_dataset_from_directory(\n",
    "    directory = train_samples,\n",
    "    labels = \"inferred\",\n",
    "    label_mode = \"binary\",\n",
    "    class_names = class_names,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    max_length = None,\n",
    "    shuffle = True,\n",
    "    seed = SEED,\n",
    "    validation_split = 0.1,\n",
    "    subset = \"validation\",\n",
    "    follow_links = False\n",
    ")\n",
    "\n",
    "class_names = np.array(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b7b15-cae4-480d-807b-11a180484459",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,y in train_dataset.take(1):\n",
    "    pass\n",
    "\n",
    "df = pl.DataFrame({\"raw\": X.numpy().astype(str)})\n",
    "print(df[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b48de-cb45-4f62-99f0-9cf125c35048",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loading CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26da7a-f0f1-4b99-b35f-519d9c9549df",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"truck-item-infos\"\n",
    "file = \"items.csv\"\n",
    "\n",
    "item_path = os.path.join(cwd, folder, file)\n",
    "items = pl.read_csv(source = item_path);\n",
    "\n",
    "items = items.lazy().with_columns([\n",
    "    (pl.col(\"Item ident\").alias(\"item_id\")),\n",
    "    (pl.col(\"Length\") / 1000), # mm to m\n",
    "    (pl.col(\"Width\") / 1000),  # mm to m\n",
    "    (pl.col(\"Height\") / 1000), # mm to m\n",
    "    (pl.col(\"Nesting height\") / 1000), # mm to m\n",
    "    (pl.col(\"Weight\") / 1000), # kg to tons\n",
    "    (pl.col(\"Forced orientation\") == \"lengthwise\").alias(\"ForcedLength\"),\n",
    "    (pl.col(\"Forced orientation\") == \"widthwise\").alias(\"ForcedWidth\"),\n",
    "    ((pl.col(\"Height\") - pl.col(\"Nesting height\")) / 1000).alias(\"NestedHeight\"),\n",
    "    pl.col(\"dataset\").str.extract(\"([A-Z])\")\n",
    "]).drop([\n",
    "    \"Forced orientation\", \"Max stackability\", \"Inventory cost\",\n",
    "    \"Earliest arrival time\", \"Latest arrival time\", \"Number of items\",\n",
    "    \"instance_id\",\n",
    "    \"Stackability code\",\n",
    "    \"Item ident\", # drop because of rename earlier\n",
    "    # Unnecassary columns\n",
    "    \"Product code\",\n",
    "    \"Package\",\n",
    "    \"Plant code\",\n",
    "    \"Product code\",\n",
    "    \"Package code\",\n",
    "    \"NestedHeight\"\n",
    "]).unique()\n",
    "\n",
    "items.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc63cfa-2e58-450c-8880-6b1bbfc96140",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"trucks.csv\"\n",
    "\n",
    "truck_path = os.path.join(cwd, folder, file)\n",
    "\n",
    "truck_clms = [\"Id truck\", \"dataset\", \"instance\",\n",
    "              \"Supplier code\", \"Supplier dock\", \"Plant dock\",\n",
    "              \"Supplier loading order\", \"Supplier dock loading order\", \"Plant dock loading order\",\n",
    "             ]\n",
    "\n",
    "truck_stops = (\n",
    "    pl.read_csv(source = truck_path, columns = truck_clms)\n",
    "    .lazy()\n",
    "    .rename({\"Id truck\": \"truck_id\"})\n",
    "    .sort([\"dataset\", \"instance\", \"truck_id\"])\n",
    "    .with_columns([pl.col(\"dataset\").str.extract(\"(\\w)\")])\n",
    "    .unique()\n",
    ")\n",
    "truck_stops.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e492480-4e8d-4534-8029-096c64ec1da1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = \"trucks.csv\"\n",
    "\n",
    "truck_path = os.path.join(cwd, folder, file)\n",
    "\n",
    "truck_clms = [\"Id truck\", \"dataset\", \"instance\",\n",
    "              \"Length\", \"Width\", \"Max weight\",\n",
    "              \"EMmm\", \"EMmr\"\n",
    "             ]\n",
    "\n",
    "truck_dims = (\n",
    "    pl.read_csv(source = truck_path, columns = truck_clms)\n",
    "    .lazy()\n",
    "    .rename({\"Id truck\": \"truck_id\", \"Max weight\": \"Weight\"})\n",
    "    .sort([\"dataset\", \"instance\", \"truck_id\"])\n",
    "    .with_columns([pl.col(\"dataset\").str.extract(\"(\\w)\")])\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "truck_dims = truck_dims.with_columns([\n",
    "    (pl.col(\"Length\") / 1000),\n",
    "    (pl.col(\"Width\") / 1000),\n",
    "    (pl.col(\"Weight\") / 1000),\n",
    "    (pl.col(\"EMmm\") / 1000),\n",
    "    (pl.col(\"EMmr\") / 1000),\n",
    "])\n",
    "\n",
    "truck_dims.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4904732-a116-4f7d-a5c1-8d17caf2a558",
   "metadata": {},
   "source": [
    "## Preprocessing Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9335a9-2bd8-41f7-ba46-909dee42efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.preprocessing import extract_raw_data, explode_instances_into_stacks, explode_stacks_into_items\n",
    "from utils.preprocessing import join_items, group_items_by_stack, join_truck_loading_order, append_truck_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f191cc73-619b-4f7e-9f1d-2ed53bca5583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XX = (\n",
    "    df.lazy()\n",
    "    .pipe(extract_raw_data)\n",
    "    .pipe(explode_instances_into_stacks)\n",
    "    .pipe(explode_stacks_into_items)\n",
    "    .pipe(join_items, items)\n",
    "    .pipe(group_items_by_stack)\n",
    "    .pipe(join_truck_loading_order, truck_stops)\n",
    "    .pipe(append_truck_info, truck_dims)\n",
    "    .collect()\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c5abbd-dbdb-49a6-a47b-a8c47618b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_representation(X, packing_clm=6):\n",
    "\n",
    "    # add columns for Length and Width Remainder\n",
    "    X = np.append(X, np.zeros((X.shape[0], 2)), axis=1)\n",
    "    \n",
    "    indices = np.unique(X[:, 0])\n",
    "    indices = np.sort(indices)\n",
    "\n",
    "    # (batch_size, ?, features)\n",
    "    X = np.array([X[X[:,0] == idx] for idx in indices], dtype = \"object\")\n",
    "\n",
    "    # replace the packing order with the stop index (i.e 1-1-1 and 1-1-2 turn to 0 and 1, respectively)\n",
    "    #packing_clm = min([i for i, clm in enumerate(df.columns) if clm == \"packing_order\"])\n",
    "    \n",
    "    for i, x in enumerate(X):\n",
    "        packing_order = x[:,packing_clm]\n",
    "        stops = np.unique(packing_order)\n",
    "        stops = np.sort(stops)\n",
    "        stops = {stop: j for j, stop in enumerate(stops)}\n",
    "        stops = [stops[order] for order in packing_order]\n",
    "        X[i][:,packing_clm] = stops\n",
    "\n",
    "    # pad the variable length number of stacks into fixed\n",
    "    #  (batch_size, pad_size, features)\n",
    "    X = tf.keras.utils.pad_sequences(X, maxlen=PAD_SIZE, padding = \"post\", dtype=\"float32\")\n",
    "    # drop the index column (batch_size, pad_len, n_features)\n",
    "    X = X[:,:,1:].astype(\"float32\")\n",
    "\n",
    "    # Add Length and width Remainder\n",
    "    for xx in X:\n",
    "        truck_width = max(xx[:,1])\n",
    "        # Length Remainder\n",
    "        xx[:,-2] = truck_width % xx[:,0]\n",
    "        # Width Remainder\n",
    "        xx[:,-1] = truck_width % xx[:,1]\n",
    "    \n",
    "    X = np.nan_to_num(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "c = get_tensor_representation(XX)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab19ec-58a3-4088-a89b-934e01616d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_labels(df: pl.DataFrame) -> dict[str: np.array]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = \"MIP Improvement - 2D Vol: \\d*\\.\\d* \\[m2\\] - packed 2D Vol Ratio\\: \\d*\\.\\d* \\[\\%\\] - after \\d*\\.\\d* \\[s\\]\"\n",
    "    mip_improvements = df[\"raw\"].str.extract_all(pattern)#.list[-1][2]\n",
    "\n",
    "    # mip_improvements: pl.Series[list[str]]\n",
    "    # with entries according to the pattern, i.e all MIP improvement rows\n",
    "    \n",
    "    y_num_improvements = mip_improvements.list.len()-1\n",
    "    \n",
    "    y_improvement = y_num_improvements > 0\n",
    "\n",
    "    y_packed_area_ratio = mip_improvements.list[-1].str.extract(\"\\: (\\d*\\.\\d*) \\[\\%\\]\").cast(pl.Float32)\n",
    "\n",
    "    y_packed_area = mip_improvements.list[-1].str.extract(\"- 2D Vol: (\\d*\\.\\d*) \\[m2\\]\").cast(pl.Float32)\n",
    "\n",
    "    y_first_update = mip_improvements.list[1].str.extract(\"- after (\\d*\\.\\d*) \\[s\\]\").cast(pl.Float32).fill_null(0)\n",
    "    \n",
    "    y_last_update = mip_improvements.list[-1].str.extract(\"- after (\\d*\\.\\d*) \\[s\\]\").cast(pl.Float32)\n",
    "\n",
    "    # missing stacks:\n",
    "    y_stack_not_included = np.zeros((len(df), PAD_SIZE), dtype=float)\n",
    "    pattern = \"Stack (\\d*) not in final solution with items:\"\n",
    "    x = df[\"raw\"].str.extract_all(pattern).map_elements(lambda x: [int(i.split(\" \")[1]) for i in x])\n",
    "    \n",
    "    for i, missing_stacks in enumerate(x):\n",
    "        for j in missing_stacks:\n",
    "            y_stack_not_included[i, j] +=1\n",
    "\n",
    "\n",
    "    y = [\n",
    "        y_improvement.to_numpy().astype(\"float32\"),\n",
    "        #y_num_improvements.to_numpy().astype(int),\n",
    "        #y_packed_area_ratio.to_numpy(),\n",
    "        #y_packed_area.to_numpy(),\n",
    "        #np.log1p(y_first_update.to_numpy()),\n",
    "        #y_last_update.to_numpy(),\n",
    "        y_stack_not_included.astype(\"float32\")\n",
    "    ]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa29a5-dc95-42ea-b83c-71eb6033d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"MIP Improvement - 2D Vol: \\d*\\.\\d* \\[m2\\] - packed 2D Vol Ratio\\: \\d*\\.\\d* \\[\\%\\] - after \\d*\\.\\d* \\[s\\]\"\n",
    "mip_improvements = df[\"raw\"].str.extract_all(pattern)#.list[-1][2]\n",
    "y_num_improvements = mip_improvements.list.len()-1\n",
    "y_improvement = y_num_improvements > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b87667-6312-4513-8242-501990e1deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stack_not_included = np.zeros((BATCH_SIZE, PAD_SIZE), dtype=float)\n",
    "pattern = \"Stack (\\d*) not in final solution with items:\"\n",
    "x = df[\"raw\"].str.extract_all(pattern).map_elements(lambda x: [int(i.split(\" \")[1]) for i in x])\n",
    "    \n",
    "for i, missing_stacks in enumerate(x):\n",
    "    for j in missing_stacks:\n",
    "        y_stack_not_included[i, j] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124bd32-2e6d-49c5-8860-26ab6cb216f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polars_transformation(X_batch, y_batch, shuffle=True) -> (np.array, np.array):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X: np.array[float32]\n",
    "        3D Feature Tensor of shape (Batch_size, Pad_size, n_features=7)\n",
    "\n",
    "        - Batch_size: Truck Optimization Instances\n",
    "        - Pad_size: Stacks (or Trucks), padded up to create tensors\n",
    "        - n_features: Length, Width, Weight, L/W Forced Orientation\n",
    "                      packing order, is_truck\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pl.DataFrame({\"raw\": X_batch.numpy().astype(str)})\n",
    "    \n",
    "    X = (\n",
    "        df.lazy()\n",
    "        .pipe(extract_raw_data)\n",
    "        .pipe(explode_instances_into_stacks)\n",
    "        .pipe(explode_stacks_into_items)\n",
    "        .pipe(join_items, items)\n",
    "        .pipe(group_items_by_stack)\n",
    "        .pipe(join_truck_loading_order, truck_stops)\n",
    "        .pipe(append_truck_info, truck_dims)\n",
    "        .collect()\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "    X = get_tensor_representation(X)\n",
    "\n",
    "    # fill final column with bool for stack not in initial solution\n",
    "    pattern = \"Stack (\\d*) missing:\"\n",
    "    x = df[\"raw\"].str.extract_all(pattern).map_elements(lambda x: [int(i.split(\" \")[1]) for i in x])\n",
    "\n",
    "    for i, missing_stacks in enumerate(x):\n",
    "        for j in missing_stacks:\n",
    "            X[i, j, 6] +=1\n",
    "\n",
    "    \n",
    "    # extract the time limit\n",
    "    pattern = \"2D Packing MIP with Time Limit (\\d*\\.?\\d*) \\[s\\]\"\n",
    "    x_time_limit = df[\"raw\"].str.extract(pattern).cast(pl.Float32).to_numpy()\n",
    "\n",
    "    y_batch = y_batch.numpy()\n",
    "    y_batch = [y_batch]\n",
    "    y_extra = get_additional_labels(df)\n",
    "\n",
    "    if shuffle:\n",
    "        idx = np.arange(X.shape[1])\n",
    "        idx = np.random.choice(idx, size=PAD_SIZE, replace=False)\n",
    "        X = X[:,idx,:]\n",
    "        y_extra[-1] = y_extra[-1][:,idx]\n",
    "\n",
    "    \n",
    "    y_batch += y_extra\n",
    "    # Drop y_batch\n",
    "    #y_batch = y_extra\n",
    "\n",
    "    X = [X, x_time_limit]\n",
    "    \n",
    "    return X, y_batch\n",
    "\n",
    "cx, cy = polars_transformation(X, y, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2235d83-51e0-4c01-84e7-c04cb712ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cx[0][0, :40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1bff52-d516-4e59-adc3-dc6879865197",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba3a13-e18c-42b5-a652-e98181cfadef",
   "metadata": {},
   "source": [
    "## Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d9d3a-8089-45e8-8510-d5692a1239f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = cx[0].shape[-1]\n",
    "\n",
    "# number of encoder blocks\n",
    "n_blocks = 3 #6\n",
    "num_heads = 3 #4\n",
    "key_dim = int(N_FEATURES/num_heads)\n",
    "use_bias = True\n",
    "upscale_factor = 1.5 # 2\n",
    "bias_regularizer = None #tf.keras.regularizers.L1(0.01),\n",
    "use_PreLN = True\n",
    "\n",
    "## Encoder dropout in total and per layer\n",
    "encoder_dropout = 0.2\n",
    "\n",
    "# activation function\n",
    "activation = \"gelu\" # \"relu\" \"gelu\" \"selu\" \"swish\"\n",
    "\n",
    "# Learning Rate\n",
    "initial_lr = 10e-3 # 10e-3\n",
    "lr_decay = 0.9995\n",
    "#decay = 0.001\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "#optimizer = keras.optimizers.SGD(learning_rate=initial_lr)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = initial_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b6560-02e0-45e1-b69a-99978203bb3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Losses and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b025e4a-1aa1-4c27-ae46-57d143ce6a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_solved = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    gamma=2,\n",
    "    alpha = 1-y.numpy().mean(),\n",
    "    name='solved_focal_loss'\n",
    ")\n",
    "\n",
    "loss_improvement = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    gamma=1.5,\n",
    "    alpha = 1-y_improvement.mean(),\n",
    "    name='improvement_focal_loss'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_stacks = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    gamma=6,\n",
    "    alpha = 1-y_stack_not_included.mean(),\n",
    "    name='stacks_focal_loss'\n",
    ")\n",
    "\n",
    "print(f\"Solved Prevalence: {y.numpy().mean():.1%}\")\n",
    "print(f\"Improvement Prevalence: {y_improvement.mean():.1%}\")\n",
    "print(f\"Stack Missing Prevalence: {y_stack_not_included.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c4de6-f775-4acb-8ac1-282e8fad9c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_objects = [\n",
    "    loss_solved,\n",
    "    loss_improvement,\n",
    "    loss_stacks\n",
    "]\n",
    "\n",
    "mean_losses = [tf.keras.metrics.Mean(name=target) for target in TARGET_LABELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e5121-bd70-407e-89eb-d7cae6c21c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics: list[list[tf.keras.metrics]] = [\n",
    "    [\n",
    "        tf.keras.metrics.Precision(name = \"PRC\"),\n",
    "        tf.keras.metrics.Recall(name = \"SNS\"),\n",
    "        tf.keras.metrics.AUC(curve='PR', name=\"AUC\"),\n",
    "        tf.keras.metrics.BinaryAccuracy(name = \"ACC\")\n",
    "    ],\n",
    "    [\n",
    "        tf.keras.metrics.Precision(name = \"PRC\"),\n",
    "        tf.keras.metrics.Recall(name = \"SNS\"),\n",
    "        tf.keras.metrics.AUC(curve='PR', name=\"AUC\"),\n",
    "        tf.keras.metrics.BinaryAccuracy(name = \"ACC\")\n",
    "    ],\n",
    "    [\n",
    "        tf.keras.metrics.Precision(name = \"PRC\"),\n",
    "        tf.keras.metrics.Recall(name = \"SNS\"),\n",
    "        tf.keras.metrics.AUC(curve='PR', name=\"AUC\"),\n",
    "        tf.keras.metrics.BinaryAccuracy(name = \"ACC\")\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2172818-ab30-45a6-a332-0062ac6574e7",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f6f99-9f5c-4a06-98d8-a9b1b199fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Masking, BatchNormalization, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.layers import Add, Dense, Input, Reshape, Permute, Lambda, Concatenate\n",
    "from keras import backend as K\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from utils.DNN.model_layers import TransformerEncoder\n",
    "\n",
    "# build a model using the functional API:\n",
    "\n",
    "# Input\n",
    "input_stack_level = Input(shape=(PAD_SIZE, N_FEATURES), name=\"StackLevelInputFeatures\")\n",
    "input_time_limit = Input(shape=(1), name=\"TimeLimitInput\")\n",
    "inputs = [input_stack_level, input_time_limit]\n",
    "\n",
    "\n",
    "# Masking padded input\n",
    "masking_layer = Masking(\n",
    "    mask_value=0,\n",
    "    input_shape=(PAD_SIZE, N_FEATURES),\n",
    "    dtype=tf.float16,\n",
    "    name=\"MaskingLayer\"\n",
    ")\n",
    "x = masking_layer(input_stack_level)\n",
    "\n",
    "#batch_norm_layer = BatchNormalization(name=\"BatchNormalizationLayer\")\n",
    "#x = batch_norm_layer(masking_layer)\n",
    "\n",
    "\n",
    "# Encoder Block\n",
    "for i in range(1, n_blocks+1):\n",
    "    # Self attention with add and layer norm\n",
    "    self_attention_layer = TransformerEncoder(\n",
    "        num_heads=num_heads, key_dim=key_dim,\n",
    "        activation=activation,\n",
    "        units = N_FEATURES,\n",
    "        upscale_factor=upscale_factor,\n",
    "        dropout=encoder_dropout,\n",
    "        use_bias=use_bias,\n",
    "        use_PreLN=use_PreLN,\n",
    "        bias_regularizer=bias_regularizer,\n",
    "        idx = i,\n",
    "    )\n",
    "    x, _ = self_attention_layer(x)\n",
    "\n",
    "final_layer_norm = LayerNormalization(name=\"FinalLayerNorm\")\n",
    "x = final_layer_norm(x)\n",
    "\n",
    "\n",
    "#self_attention_layer_solved = TransformerEncoder(\n",
    "#    num_heads=num_heads, key_dim=key_dim,\n",
    "#    activation=activation,\n",
    "#    units = N_FEATURES,\n",
    "#    upscale_factor=upscale_factor,\n",
    "#    dropout=encoder_dropout,\n",
    "#    use_bias=use_bias,\n",
    "#    use_PreLN=use_PreLN,\n",
    "#    bias_regularizer=bias_regularizer,\n",
    "#    idx=\"Solved\"\n",
    "#)\n",
    "#attention_solved, _ = self_attention_layer_solved(x)\n",
    "#attention_solved = final_layer_norm(attention_solved)\n",
    "\n",
    "#self_attention_layer_improvement = TransformerEncoder(\n",
    "#    num_heads=num_heads, key_dim=key_dim,\n",
    "#    activation=activation,\n",
    "#    units = N_FEATURES,\n",
    "#    upscale_factor=upscale_factor,\n",
    "#    dropout=encoder_dropout,\n",
    "#    use_bias=use_bias,\n",
    "#    use_PreLN=use_PreLN,\n",
    "#    bias_regularizer=bias_regularizer,\n",
    "#    idx=\"Improvement\",\n",
    "#)\n",
    "#attention_improvement, _ = self_attention_layer_improvement(x)\n",
    "#attention_improvement = final_layer_norm(attention_improvement)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# After Attention, reduce to single dimension\n",
    "add_across_dim = Lambda(\n",
    "    lambda x: K.sum(x, axis=1)/PAD_SIZE**1,\n",
    "    output_shape=lambda s: (s[0], s[2]),\n",
    "    name = \"ReduceStackDimensionViaSummation\"\n",
    ")\n",
    "\n",
    "#attention_solved = add_across_dim(attention_solved)\n",
    "#attention_improvement = add_across_dim(attention_improvement)\n",
    "\n",
    "\n",
    "\n",
    "reshape = Lambda(lambda x: tf.squeeze(x), name=\"Output\")\n",
    "\n",
    "# Pipe Attention directly into missing stack prediction:\n",
    "\n",
    "output_stacks = Dense(1, activation='sigmoid', name = \"PredictionStacks\")\n",
    "xx = output_stacks(x)\n",
    "output_stacks = reshape(xx)\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the Time Limit\n",
    "# - The time limit ranges from 0 to 30,\n",
    "# - (tl - 15)/5 should roughly normalize\n",
    "\n",
    "normalize_time_limit = Lambda(\n",
    "    lambda x: (x-15)/5,\n",
    "    name = \"StandardizeTimeLimit\"\n",
    ")\n",
    "input_time_limit = normalize_time_limit(input_time_limit)\n",
    "concatenate_layer = Concatenate(axis=-1, name=f\"ConcatenateLayer\")\n",
    "#x = concatenate_layer([x, input_time_limit])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = add_across_dim(x)\n",
    "\n",
    "\n",
    "xx = concatenate_layer([x, input_time_limit])\n",
    "#xx = concatenate_layer([attention_solved, input_time_limit])\n",
    "fully_connected_layer_solved = Dense(units=N_FEATURES+1, activation=activation,name=f\"FullyConnectedLayerSolved\")\n",
    "xx = fully_connected_layer_solved(xx)\n",
    "output_solved = Dense(1, activation='sigmoid', name = \"PredictionSolved\")\n",
    "xx = output_solved(xx)\n",
    "output_solved = reshape(xx)\n",
    "\n",
    "\n",
    "xx = concatenate_layer([x, input_time_limit])\n",
    "#xx = concatenate_layer([attention_improvement, input_time_limit])\n",
    "fully_connected_layer_improvement = Dense(units=N_FEATURES+1, activation=activation,name=f\"FullyConnectedLayerImprovement\")\n",
    "xx = fully_connected_layer_improvement(xx)\n",
    "output_improvement = Dense(1, activation='sigmoid', name = \"PredictionImprovement\")\n",
    "xx = output_improvement(xx)\n",
    "output_improvement = reshape(xx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outputs = [\n",
    "    output_solved,\n",
    "    output_improvement,\n",
    "    output_stacks\n",
    "]\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbff812-25bb-4ea0-a14a-048cbc6da1ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model Architecture Inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0313b-3fe0-498b-925f-3547fcd860af",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model(cx) # just check if it works\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf228a55-a71a-42ac-bf9c-8df8bc258ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    #to_file,\n",
    "    show_shapes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dfadaf-029c-49d3-81f3-664967ff38b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training Process Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911b3da-221c-4414-9358-7fee5a42a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_decay(epoch: int, initial_lr:float=10e-3, lr_decay:float=0.9) -> float:\n",
    "    \"\"\"\n",
    "    Exponential decay learning rate schedule\n",
    "    \"\"\"\n",
    "    return initial_lr * lr_decay**epoch\n",
    "\n",
    "#xx = np.arange(5000)\n",
    "#yy = geometric_decay(xx, lr_decay=0.9995)\n",
    "#plt.plot(xx, yy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf804408-85ea-4824-8a74-fb9feb1807f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, losses, all_metrics=None, training:bool=True):\n",
    "    tabs = \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\"\n",
    "    metrics = tabs.join([\n",
    "        f\"{TARGET_LABELS[i]:<12}\\tLoss: {losses[i].result():.4f}\" +\n",
    "        \"\\t\".join([\n",
    "            f\"{m.name:>10}: {m.result():.2%}\"\n",
    "            if m.name not in [\"MAE\", \"RMSE\"]\n",
    "            else f\"{m.name:>10}: {m.result():.4f}\"\n",
    "            for m in metrics\n",
    "        ])\n",
    "        for i, metrics in enumerate(all_metrics)\n",
    "    ])\n",
    "    \n",
    "    if training:\n",
    "        prefix=\"Training\"\n",
    "    else:\n",
    "        prefix=\"Validation\"\n",
    "    \n",
    "    print(f\"\\r{prefix}-Iteration: {iteration+1:0>3}/{total:<3}\\tLR: {float(optimizer.lr):<1000.9f}\" + metrics,\n",
    "          end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f44dd2-a301-4c37-919b-5a93fbe6c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def model_pass(inputs, targets, training):\n",
    "    \"\"\"\n",
    "    Usual Tensorflow model passing of inputs throught the network.\n",
    "    If in training mode, the optimizier can apply the gradients\n",
    "    observed with GradientTape to the model parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        outputs = model(inputs, training=training)\n",
    "        losses = [l(t, o) for l,o,t in zip(loss_objects, outputs, targets)]\n",
    "        \n",
    "        if training:\n",
    "            gradients = tape.gradient(losses, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "        for i, mean_loss in enumerate(mean_losses):\n",
    "            mean_loss(losses[i])\n",
    "        \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d2a8e-ff78-42a9-871c-fabbc1da5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataset, all_metrics:list[list[tf.keras.metrics]],\n",
    "              epoch:int, history:pd.DataFrame, training:bool=True,\n",
    "              shuffle:bool=True):\n",
    "    \"\"\"\n",
    "    Single epoch, running several training steps over a dataset\n",
    "    \"\"\"\n",
    "\n",
    "    for step, (X_batch, y_batch) in enumerate(dataset):\n",
    "        \n",
    "        X_batch, y_batch = polars_transformation(X_batch, y_batch, shuffle=shuffle)\n",
    "        \n",
    "        outputs = model_pass(X_batch, y_batch, training=training)\n",
    "        #mean_loss(loss)\n",
    "        \n",
    "        for i, metrics in enumerate(all_metrics):\n",
    "            for metric in metrics:\n",
    "                metric(y_batch[i].reshape(-1), outputs[i].numpy().reshape(-1))\n",
    "    \n",
    "        print_status_bar(step, len(dataset), mean_losses, all_metrics, training=training)\n",
    "    \n",
    "        #### LEARNING RATE UPDATE ####\n",
    "        if training:\n",
    "            optimizer.lr = geometric_decay((epoch-1)*len(dataset)+step, initial_lr, lr_decay)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    #### UPDATE METRICS\n",
    "    for i, metrics in enumerate(all_metrics):\n",
    "        for metric in metrics:\n",
    "            data = pd.DataFrame({\n",
    "                \"Epoch\": [epoch],\n",
    "                \"Target\": [TARGET_LABELS[i]],\n",
    "                \"Metric\": [metric.name if training else f\"Val-\"+metric.name],\n",
    "                \"Value\": [float(metric.result())],\n",
    "            })\n",
    "            history = pd.concat([history, data])\n",
    "            metric.reset_states()\n",
    "\n",
    "    for i, mean_loss in enumerate(mean_losses):\n",
    "        data = pd.DataFrame({\n",
    "            \"Epoch\": [epoch],\n",
    "            \"Target\": [TARGET_LABELS[i]],\n",
    "            \"Metric\": [\"Loss\" if training else f\"Val-\"+\"Loss\"],\n",
    "            \"Value\": [float(mean_loss.result())],\n",
    "        })\n",
    "        history = pd.concat([history, data])\n",
    "        mean_loss.reset_states()\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ef915-1e01-42dd-b423-71593e828fcf",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8175650-b296-4222-b028-ecb715c458c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clms = [\"Epoch\", \"Target\", \"Metric\", \"Value\"]\n",
    "track = pd.DataFrame(columns = clms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464563b0-8ebe-4448-9105-a25ed5e36383",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    print(f\"Epoch {epoch:0>3}/{ n_epochs:0>3}\")\n",
    "\n",
    "    track = run_epoch(train_dataset, all_metrics, epoch=epoch, history=track, training=True, shuffle=True)\n",
    "    track = run_epoch(val_dataset, all_metrics, epoch=epoch, history=track, training=False, shuffle=True)\n",
    "\n",
    "    # make an intermediate save of the model\n",
    "    model_path = os.path.join(cwd, \"models\", f\"EncoderUpdate-{epoch}.tf\")\n",
    "    model.save(model_path, overwrite=True, save_format=\"tf\")\n",
    "\n",
    "    print(\"-\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c961887-1831-43e7-98a7-51b71bf3bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = track\\\n",
    "    .pivot_table(\n",
    "        values = [\"Value\"],\n",
    "        columns=[\"Metric\"],\n",
    "        index=[\"Epoch\", \"Target\"]\n",
    "    )\\\n",
    "    .reset_index()\\\n",
    "    .set_index(\"Epoch\", drop=True)\n",
    "\n",
    "clms = history.columns\n",
    "history.columns = [clm[i>=1] for i, clm in enumerate(clms)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb1501-21f7-4ef2-96a7-af4205bf0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "clms = history.columns\n",
    "\n",
    "fig, ax = plt.subplots(len(TARGET_LABELS), 2, figsize=(16, 9*len(TARGET_LABELS)))\n",
    "\n",
    "colors = [\"cyan\", \"orange\", \"blue\", \"forestgreen\", \"red\", \"purple\"]\n",
    "\n",
    "for i, target in enumerate(TARGET_LABELS):\n",
    "    history.loc[(history.Target == target), (~clms.str.contains(\"Val-\"))&(~clms.str.contains(\"Loss\"))]\\\n",
    "        .plot(ax=ax[i, 0], ls = \"--\", color = colors, label = \"Training\")\n",
    "    history.loc[(history.Target == target), (clms.str.contains(\"Val-\"))&(~clms.str.contains(\"Loss\"))]\\\n",
    "        .plot(ax=ax[i, 0], lw = 3, color = colors, label = \"Validation\")\n",
    "    ax[i, 0].set(ylabel = target)\n",
    "\n",
    "    # Plot the loss\n",
    "    history.loc[(history.Target == target), (~clms.str.contains(\"Val-\"))&(clms.str.contains(\"Loss\"))]\\\n",
    "        .plot(ax=ax[i, 1], ls = \"--\", color = \"red\", label = \"Training\")\n",
    "    history.loc[(history.Target == target), (clms.str.contains(\"Val-\"))&(clms.str.contains(\"Loss\"))]\\\n",
    "        .plot(ax=ax[i, 1], lw = 3, color = \"red\", label = \"Validation\")\n",
    "    ax[i, 1].set(yscale = \"log\")\n",
    "\n",
    "ax[0, 0].set(title=\"Metrics\")\n",
    "ax[0, 1].set(title=\"Loss\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb1d76-f599-4e84-97b5-2c0c76f86adf",
   "metadata": {},
   "source": [
    "Tradeoff between Sensitivity and Precision can be seen very good here.\n",
    "Most likely explanation:\n",
    "- the bias of the final layer before the sigmoid has changed drastically.\n",
    "- if the bias increases, predicted probabilities increase. This means more found solved cases and higher Sensitivity\n",
    "- if the bias decreases, predicted probabilities decrease. This means less found solved cases, only the more certain cases. Higher Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808e5f5-47fc-49e1-b1b7-7ad7a4e7a3f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Visualize Attention Scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f185a32-7dcb-42fc-9a63-1d90aac13633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.DNN.utils import get_attention_scores\n",
    "\n",
    "attention = get_attention_scores(model, model_inputs=cx, layer_name=\"transformer_encoder_15\")\n",
    "attention.shape\n",
    "#cx[0][24].round(2)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377285ec-90de-4f20-8823-ca2b52c1d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance, head = 24, 3 # 18: 36 # 24 is a very simple but good example\n",
    "\n",
    "attention_matrix = attention[instance, head, :, :]\n",
    "\n",
    "# truncate:\n",
    "n_inputs = np.argmax(np.all(cx[0][instance].round(2) == 0, axis=1))\n",
    "attention_matrix = attention_matrix[:n_inputs, :n_inputs]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(attention_matrix.T,\n",
    "            square=True,\n",
    "            #vmin=0, vmax=1\n",
    "           );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68e4b5-2601-433d-9910-212c9dbee80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [28, 24, 25, 26]\n",
    "heads = list(range(num_heads))\n",
    "\n",
    "fig, ax = plt.subplots(len(instances), num_heads, figsize=(16, len(instances)*5))\n",
    "\n",
    "for i, instance in enumerate(instances):\n",
    "    for h, head in enumerate(heads):\n",
    "\n",
    "        # Truncate\n",
    "        attention_matrix = attention[instance, head, :, :]\n",
    "        n_inputs = np.argmax(np.all(cx[0][instance].round(2) == 0, axis=1))\n",
    "        attention_matrix = attention_matrix[:n_inputs, :n_inputs]\n",
    "        \n",
    "        sns.heatmap(\n",
    "            attention_matrix.T,\n",
    "            square=True,\n",
    "            cbar=False,\n",
    "            #vmin=0, vmax=0.1,\n",
    "            ax=ax[i, h]\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0a478-caf6-4a26-a06f-5b0507c62ceb",
   "metadata": {},
   "source": [
    "# Load the model to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0150bde-1688-4a1d-822b-12088e32045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(cwd, \"models\", f\"EncoderUpdate-20.tf\")\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(model_path)\n",
    "y_pred = loaded_model(cx);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd136e-0080-4029-93fa-0b59d98db727",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = data=pd.DataFrame([y_pred[1].numpy(), cy[1]]).T\n",
    "pred_df.columns = [\"prediction\", \"improvement\"]\n",
    "\n",
    "sns.histplot(\n",
    "    data=pred_df,\n",
    "    x=\"prediction\",\n",
    "    hue=\"improvement\",\n",
    "    bins=np.arange(0, 1, 0.02)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519120a-8b78-49df-8403-88d108a56831",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = keras.preprocessing.text_dataset_from_directory(\n",
    "    directory = test_samples,\n",
    "    labels = \"inferred\",\n",
    "    label_mode = \"binary\",\n",
    "    class_names = list(class_names),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    max_length = None,\n",
    "    shuffle = True,\n",
    "    seed = SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b1039-2be9-42c4-8dcb-d0f6651c9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_epoch(test_dataset, all_metrics, epoch=epoch, history=track, training=False, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
